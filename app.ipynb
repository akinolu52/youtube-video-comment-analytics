{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:44:25.474514Z",
     "start_time": "2025-02-26T18:44:25.472276Z"
    }
   },
   "cell_type": "code",
   "source": "print('here..')",
   "id": "79a62803a91aac02",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here..\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T07:56:10.491281Z",
     "start_time": "2025-02-26T07:56:10.489633Z"
    }
   },
   "cell_type": "code",
   "source": "# !pip install google-api-python-client pandas vaderSentiment textblob python-dotenv spacy nltk wordcloud scikit-learn",
   "id": "1ad45d8261d46969",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# !pip install spacy\n",
    "# from spacy.cli import download\n",
    "#\n",
    "# download(\"en_core_web_sm\")"
   ],
   "id": "53ee712ba1694dcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:48:28.337688Z",
     "start_time": "2025-02-26T18:48:13.380346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.discovery import build\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "load_dotenv()"
   ],
   "id": "c519ff68a5843a0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:48:36.492332Z",
     "start_time": "2025-02-26T18:48:36.477743Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class YouTubeDataFetcher:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the YouTubeDataFetcher object with the API key and output file.\n",
    "        \"\"\"\n",
    "        self.api_keys = [\n",
    "            os.getenv('YOUTUBE_API_KEY_1'),\n",
    "            os.getenv('YOUTUBE_API_KEY_2'),\n",
    "            os.getenv('YOUTUBE_API_KEY_3'),\n",
    "            os.getenv('YOUTUBE_API_KEY_4'),\n",
    "            os.getenv('YOUTUBE_API_KEY_5'),\n",
    "        ]\n",
    "        self.output_file = 'youtube_data.csv'\n",
    "        self.current_api_key_index = 0  # Keeps track of the current API key index\n",
    "        self.youtube = self.get_youtube_client(self.api_keys[self.current_api_key_index])\n",
    "        self.headers = [\"video_id\", \"title\", \"description\", \"view_count\", \"like_count\",\n",
    "                        \"dislike_count\", \"comment_count\", \"duration\", \"favorite_count\", \"comments\", \"sentiment_score\"]\n",
    "\n",
    "        # Load existing processed video IDs\n",
    "        self.processed_video_ids = self.load_processed_video_ids()\n",
    "\n",
    "        # Initialize CSV file (with headers) if it doesn't exist\n",
    "        self.initialize_csv()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_youtube_client(api_key):\n",
    "        \"\"\"\n",
    "        Initializes and returns the YouTube API client with the given API key.\n",
    "        \"\"\"\n",
    "        return build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    def load_processed_video_ids(self):\n",
    "        \"\"\"\n",
    "        Loads the list of processed video IDs from the output CSV file.\n",
    "        Returns a set of video IDs that have already been processed.\n",
    "        \"\"\"\n",
    "        processed_ids = set()\n",
    "        try:\n",
    "            df = pd.read_csv(self.output_file)\n",
    "            processed_ids = set(df['video_id'].values)\n",
    "        except FileNotFoundError:\n",
    "            pass  # If the file doesn't exist yet, just return an empty set\n",
    "        return processed_ids\n",
    "\n",
    "    def initialize_csv(self):\n",
    "        \"\"\"\n",
    "        Initializes the CSV file if it doesn't exist, and writes the header.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.output_file) or os.stat(self.output_file).st_size == 0:\n",
    "            # Create CSV and write the header if the file doesn't exist or is empty\n",
    "            try:\n",
    "                with open(self.output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "                    writer = csv.DictWriter(file, fieldnames=self.headers)\n",
    "                    writer.writeheader()\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating file: {str(e)}\")\n",
    "\n",
    "    def fetch_video_data(self, video_id):\n",
    "        \"\"\"\n",
    "        Fetches the video details (views, likes, etc.) and comments for the given video_id.\n",
    "        \"\"\"\n",
    "        attempt = 0\n",
    "        max_attempts = len(self.api_keys) * 2\n",
    "\n",
    "        while attempt < max_attempts:\n",
    "            api_key = self.api_keys[self.current_api_key_index % len(self.api_keys)]  # Rotate API keys continuously\n",
    "            self.youtube = self.get_youtube_client(api_key)  # Re-initialize the client on each key switch\n",
    "            try:\n",
    "                # Get the video details (view count, like count, etc.)\n",
    "                video_response = self.youtube.videos().list(\n",
    "                    part=\"snippet,statistics,contentDetails\",\n",
    "                    id=video_id\n",
    "                ).execute()\n",
    "\n",
    "                if not video_response[\"items\"]:\n",
    "                    raise Exception(f\"No video found for ID: {video_id}\")\n",
    "\n",
    "                video_info = video_response[\"items\"][0]\n",
    "                title = video_info[\"snippet\"][\"title\"]\n",
    "                description = video_info[\"snippet\"].get(\"description\", \"No description\")\n",
    "                view_count = int(video_info[\"statistics\"].get(\"viewCount\", 0))\n",
    "                like_count = int(video_info[\"statistics\"].get(\"likeCount\", 0))\n",
    "                dislike_count = int(video_info[\"statistics\"].get(\"dislikeCount\", 0))\n",
    "                comment_count = int(video_info[\"statistics\"].get(\"commentCount\", 0))\n",
    "                duration = video_info[\"contentDetails\"][\"duration\"]\n",
    "                favorite_count = int(video_info[\"statistics\"].get(\"favoriteCount\", 0))\n",
    "\n",
    "                # Fetching comments (max 100 comments)\n",
    "                comments = []\n",
    "                comment_response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=100\n",
    "                ).execute()\n",
    "\n",
    "                for item in comment_response[\"items\"]:\n",
    "                    comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                    comments.append(comment_text)\n",
    "\n",
    "                return {\n",
    "                    \"video_id\": video_id,\n",
    "                    \"title\": title,\n",
    "                    \"description\": description,\n",
    "                    \"view_count\": view_count,\n",
    "                    \"like_count\": like_count,\n",
    "                    \"dislike_count\": dislike_count,\n",
    "                    \"comment_count\": comment_count,\n",
    "                    \"duration\": duration,\n",
    "                    \"favorite_count\": favorite_count,\n",
    "                    \"comments\": comments\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                if \"quotaExceeded\" in str(e):\n",
    "                    print(f\"API key {api_key} quota exceeded. Switching to next API key.\")\n",
    "                    attempt += 1\n",
    "                    self.current_api_key_index += 1\n",
    "                    continue  # Try with the next API key\n",
    "                else:\n",
    "                    print(f\"Error fetching data for {video_id}: {str(e)}\")\n",
    "                    return None\n",
    "\n",
    "        print(\"All API keys exhausted, cannot fetch data.\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def analyze_sentiment(comments):\n",
    "        \"\"\"\n",
    "        Analyzes the sentiment of the provided comments using VADER Sentiment Analysis.\n",
    "        Returns a sentiment score.\n",
    "        \"\"\"\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        sentiment_score = 0\n",
    "        num_comments = len(comments)\n",
    "\n",
    "        for comment in comments:\n",
    "            sentiment = analyzer.polarity_scores(comment)\n",
    "            sentiment_score += sentiment['compound']\n",
    "\n",
    "        # Calculate the average sentiment score\n",
    "        if num_comments > 0:\n",
    "            sentiment_score /= num_comments\n",
    "        return sentiment_score\n",
    "\n",
    "    def save_to_csv(self, video_data):\n",
    "        \"\"\"\n",
    "        Saves the fetched video data into the CSV file, appending to it.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=self.headers)\n",
    "                writer.writerow({\n",
    "                    \"video_id\": video_data[\"video_id\"],\n",
    "                    \"title\": video_data[\"title\"],\n",
    "                    \"description\": video_data[\"description\"],\n",
    "                    \"view_count\": video_data[\"view_count\"],\n",
    "                    \"like_count\": video_data[\"like_count\"],\n",
    "                    \"dislike_count\": video_data[\"dislike_count\"],\n",
    "                    \"comment_count\": video_data[\"comment_count\"],\n",
    "                    \"duration\": video_data[\"duration\"],\n",
    "                    \"favorite_count\": video_data[\"favorite_count\"],\n",
    "                    \"comments\": \" | \".join(video_data[\"comments\"]),\n",
    "                    \"sentiment_score\": video_data[\"sentiment_score\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data for {video_data['video_id']}: {str(e)}\")\n",
    "\n",
    "    def fetch_and_process_data(self, df):\n",
    "        \"\"\"\n",
    "        Fetches and processes data for each video in the provided DataFrame.\n",
    "        Saves the results into a CSV file.\n",
    "        \"\"\"\n",
    "        for index, row in df.iterrows():\n",
    "            video_id = row['youtubeId']\n",
    "\n",
    "            # Skip the video if it's already processed\n",
    "            if video_id in self.processed_video_ids:\n",
    "                print(f\"{index}. Skipping already processed video: {video_id}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"{index}. Processing video: {video_id}\")\n",
    "\n",
    "            # Fetch video data\n",
    "            video_data = self.fetch_video_data(video_id)\n",
    "\n",
    "            if video_data:\n",
    "                # Analyze sentiment of the comments\n",
    "                sentiment_score = self.analyze_sentiment(video_data[\"comments\"])\n",
    "                video_data[\"sentiment_score\"] = sentiment_score\n",
    "\n",
    "                # Save the data to the CSV file\n",
    "                self.save_to_csv(video_data)\n",
    "\n",
    "            # Sleep to avoid rate-limiting\n",
    "            time.sleep(3)\n",
    "\n",
    "        print(f\"Data fetching and saving complete. All data saved in {self.output_file}.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_wordcloud(df):\n",
    "        \"\"\"\n",
    "        Generates a word cloud from the comments of all videos.\n",
    "        \"\"\"\n",
    "        all_comments = \" \".join(df[\"comments\"].dropna())\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_comments)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_graphs(df):\n",
    "        \"\"\"\n",
    "        Generates various plots for video statistics using Plotly Express.\n",
    "        \"\"\"\n",
    "        # Top 10 most viewed videos\n",
    "        top_10 = df.nlargest(10, \"view_count\")\n",
    "        fig = px.bar(top_10, x='title', y='view_count', title='Top 10 Most Viewed Videos')\n",
    "        fig.show()\n",
    "\n",
    "        # Bottom 10 least viewed videos\n",
    "        bottom_10 = df.nsmallest(10, \"view_count\")\n",
    "        fig = px.bar(bottom_10, x='title', y='view_count', title='Bottom 10 Least Viewed Videos')\n",
    "        fig.show()\n",
    "\n",
    "        # Most liked video\n",
    "        most_liked = df.loc[df['like_count'].idxmax()]\n",
    "        fig = px.bar(x=[most_liked['title']], y=[most_liked['like_count']], title='Most Liked Video')\n",
    "        fig.show()\n",
    "\n",
    "        # Least liked video\n",
    "        least_liked = df.loc[df['like_count'].idxmin()]\n",
    "        fig = px.bar(x=[least_liked['title']], y=[least_liked['like_count']], title='Least Liked Video')\n",
    "        fig.show()\n",
    "\n",
    "        # Video with the highest duration\n",
    "        highest_duration = df.loc[df['duration'].idxmax()]\n",
    "        fig = px.bar(x=[highest_duration['title']], y=[highest_duration['duration']],\n",
    "                     title='Video with Highest Duration')\n",
    "        fig.show()\n",
    "\n"
   ],
   "id": "12e12cbab0984320",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:48:44.909598Z",
     "start_time": "2025-02-26T18:48:44.893344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "This Clas handles the text in the dataset\n",
    "- it's used to a manipulate textual data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TextWrangler:\n",
    "    def __init__(self):\n",
    "        self.count = CountVectorizer()\n",
    "        self.tf_idf_ = TfidfVectorizer()\n",
    "        self.spacy_en = spacy.load('en_core_web_sm')\n",
    "        # Download stopwords if not already downloaded\n",
    "        try:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        except LookupError:\n",
    "            import nltk\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('punkt')\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_digit(text: str) -> str:\n",
    "        \"\"\"\n",
    "        This method removes digit from a text\n",
    "        \"\"\"\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuations(text: str) -> str:\n",
    "        \"\"\"\n",
    "        This method removes punctuation from a text\n",
    "        \"\"\"\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean the text and remove punctuations.\n",
    "        \"\"\"\n",
    "        text = re.sub('<.*?>', '', text)\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        return text.lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def word_lengthening(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Fix a word by reducing any character repeated three or more times\n",
    "        to just two instances of that character.\n",
    "        \"\"\"\n",
    "        return re.sub(r'(.)\\1{2,}', r'\\1\\1', text.strip())\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize the given text into words.\n",
    "        \"\"\"\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def remove_stopwords(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove stop words from a given text.\n",
    "        \"\"\"\n",
    "        filtered_text = [word for word in self.tokenize(text) if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_text)\n",
    "\n",
    "    def lemmatizer(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        This method lemmatizes a text by using the spaCy libary\n",
    "        \"\"\"\n",
    "        text = self.spacy_en(text)\n",
    "\n",
    "        # get the lemmatized token from text\n",
    "        lemmatized_tokens = [token.lemma_ for token in text]\n",
    "\n",
    "        # use lemmatized token to form a sentence\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_word_cloud(data, title):\n",
    "        \"\"\"\n",
    "        Generate a graphical representation of the words in the text\n",
    "        \"\"\"\n",
    "        wordcloud = WordCloud(width=900, height=600, max_words=100, background_color='white').generate(data)\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(title, fontsize=15)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        return\n"
   ],
   "id": "e74342f968c1b8b9",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:48:57.661576Z",
     "start_time": "2025-02-26T18:48:57.659764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# [os.getenv('YOUTUBE_API_KEY_1'), os.getenv('YOUTUBE_API_KEY_2'), os.getenv('YOUTUBE_API_KEY_3'),\n",
    "#  os.getenv('YOUTUBE_API_KEY_4')]"
   ],
   "id": "d955def439f2766",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:49:00.677574Z",
     "start_time": "2025-02-26T18:49:00.628117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the CSV containing the video data\n",
    "df = pd.read_csv('vdoLinks.csv')"
   ],
   "id": "5b57ee90f467e959",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:49:01.562460Z",
     "start_time": "2025-02-26T18:49:01.554122Z"
    }
   },
   "cell_type": "code",
   "source": "df.head()",
   "id": "c1af92f63e966101",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     youtubeId  movieId                               title\n",
       "0  K26_sDKnvMU        1                    Toy Story (1995)\n",
       "1  3LPANjHlPxo        2                      Jumanji (1995)\n",
       "2  rEnOoWs3FuA        3             Grumpier Old Men (1995)\n",
       "3  j9xml1CxgXI        4            Waiting to Exhale (1995)\n",
       "4  ltwvKLnj1B4        5  Father of the Bride Part II (1995)"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>youtubeId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K26_sDKnvMU</td>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3LPANjHlPxo</td>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rEnOoWs3FuA</td>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>j9xml1CxgXI</td>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ltwvKLnj1B4</td>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:49:04.690174Z",
     "start_time": "2025-02-26T18:49:04.686912Z"
    }
   },
   "cell_type": "code",
   "source": "df.shape",
   "id": "1ab473760ce8db77",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25623, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:49:08.002850Z",
     "start_time": "2025-02-26T18:49:07.973810Z"
    }
   },
   "cell_type": "code",
   "source": "df.info()",
   "id": "e9d0280659a31dbc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25623 entries, 0 to 25622\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   youtubeId  25623 non-null  object\n",
      " 1   movieId    25623 non-null  int64 \n",
      " 2   title      25623 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 600.7+ KB\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:50:57.319697Z",
     "start_time": "2025-02-26T18:50:57.307786Z"
    }
   },
   "cell_type": "code",
   "source": "df.columns",
   "id": "f3acc2bb45ac5914",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['youtubeId', 'movieId', 'title'], dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create an instance of YouTubeDataFetcher and process the data\n",
    "fetcher = YouTubeDataFetcher()\n",
    "fetcher.fetch_and_process_data(df)"
   ],
   "id": "3d026a6e176d98d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the CSV containing the video data\n",
    "df = pd.read_csv('./youtube_data.csv')"
   ],
   "id": "6811e1637d8a387b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head()",
   "id": "9ef9024096a04728"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.shape",
   "id": "a0e66fc7224ee8d5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.info()",
   "id": "7b60bc729939901f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.columns",
   "id": "681320f86692c1bc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.describe()",
   "id": "3de72f1d12e57af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['comment'].head(1)",
   "id": "52c324c1bd220c2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Data Cleaning",
   "id": "ad57712cf8e9d20c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Remove rows with missing values\n",
    "df.dropna(inplace=True)"
   ],
   "id": "713092c3183b4382"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "text_wrangler = TextWrangler()",
   "id": "5c241017136894c0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['cleanedComments'] = (df['comments']\n",
    "                         .apply(text_wrangler.remove_digit)\n",
    "                         .apply(text_wrangler.remove_punctuations)\n",
    "                         .apply(text_wrangler.clean_text)\n",
    "                         .apply(text_wrangler.word_lengthening)\n",
    "                         .apply(text_wrangler.remove_stopwords)\n",
    "                         .apply(text_wrangler.lemmatizer))\n",
    "\n",
    "df['comment'].head(1)"
   ],
   "id": "a0f2d1ac1e646622"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GenerateGrams:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_ngrams(tokens, n):\n",
    "        ngrams_list = list(ngrams(tokens, n))\n",
    "\n",
    "        return [\" \".join(_gram) for _gram in ngrams_list]\n",
    "\n",
    "    def generate_grams(self, col_df, size):\n",
    "        ngrams_list = []\n",
    "        for _data in col_df:\n",
    "            tokens = _data.split()\n",
    "            if len(tokens) <= size:\n",
    "                continue\n",
    "            ngrams_list.extend(self, self.generate_ngrams(tokens, size))\n",
    "\n",
    "        cnt_ngram = Counter(ngrams_list)\n",
    "        most_common_ngrams = cnt_ngram.most_common(15)\n",
    "\n",
    "        temp_df = pd.DataFrame(most_common_ngrams, columns=['words', 'count'])\n",
    "        temp_df = temp_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def plot_n_grams(self, col_df):\n",
    "        unigrams = self.generate_grams(col_df, 1)\n",
    "        bigrams = self.generate_grams(col_df, 2)\n",
    "        trigrams = self.generate_grams(col_df, 3)\n",
    "\n",
    "        # Set plot figure size\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 25))  # 3 rows, 1 column\n",
    "\n",
    "        # Plot Unigrams\n",
    "        ax1.barh(np.arange(len(unigrams['words'])), unigrams['count'], align='center', alpha=.5)\n",
    "        ax1.set_title('Unigrams')\n",
    "        ax1.set_yticks(np.arange(len(unigrams['words'])))\n",
    "        ax1.set_yticklabels(unigrams['words'])\n",
    "        ax1.set_xlabel('Count')\n",
    "\n",
    "        # Plot Bigrams\n",
    "        ax2.barh(np.arange(len(bigrams['words'])), bigrams['count'], align='center', alpha=.5)\n",
    "        ax2.set_title('Bigrams')\n",
    "        ax2.set_yticks(np.arange(len(bigrams['words'])))\n",
    "        ax2.set_yticklabels(bigrams['words'])\n",
    "        ax2.set_xlabel('Count')\n",
    "\n",
    "        # Plot Trigrams\n",
    "        ax3.barh(np.arange(len(trigrams['words'])), trigrams['count'], align='center', alpha=.5)\n",
    "        ax3.set_title('Trigrams')\n",
    "        ax3.set_yticks(np.arange(len(trigrams['words'])))\n",
    "        ax3.set_yticklabels(trigrams['words'])\n",
    "        ax3.set_xlabel('Count')\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout for better spacing\n",
    "        plt.show()"
   ],
   "id": "8a1cd43fa18d1dfb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Initialize the class\n",
    "gram_generator = GenerateGrams()\n",
    "\n",
    "# Generate n-grams and plot them\n",
    "gram_generator.plot_n_grams(df['cleanedComments'])"
   ],
   "id": "32f6335a215fd203"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "TextSentimentAnalysis Class provides the sentiment for the input text in its raw form\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TextSentimentAnalysis:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize vader sentiment analyzer\n",
    "        \"\"\"\n",
    "        self.sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def get_sentiment_via_vader(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        return the compound score\n",
    "        \"\"\"\n",
    "        # Using compound score as a single sentiment score\n",
    "        return self.sid.polarity_scores(text)['compound']"
   ],
   "id": "973857230a4d172"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "text_sentiment_analysis = TextSentimentAnalysis()",
   "id": "9251b515f6132240"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['commentsSentiment'] = df['comments'].apply(text_sentiment_analysis.get_sentiment_via_vader)",
   "id": "b389566ba2b11b23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df['cleanedCommentsSentiment'] = df['cleanedComments'].apply(text_sentiment_analysis.get_sentiment_via_vader)",
   "id": "b66a12e887343511"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df.head()",
   "id": "b867df57e4345051"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# generate the word cloud and plots\n",
    "fetcher.generate_wordcloud(df)"
   ],
   "id": "86f4be01d706f7f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "fetcher.generate_graphs(df)",
   "id": "df0f392c7d846432"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (yenv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
