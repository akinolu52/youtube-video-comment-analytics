{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a62803a91aac02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:44:25.474514Z",
     "start_time": "2025-02-26T18:44:25.472276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here..\n"
     ]
    }
   ],
   "source": [
    "print('here..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad45d8261d46969",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T07:56:10.491281Z",
     "start_time": "2025-02-26T07:56:10.489633Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install google-api-python-client pandas vaderSentiment textblob python-dotenv spacy nltk wordcloud scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee712ba1694dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# from spacy.cli import download\n",
    "#\n",
    "# download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c519ff68a5843a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:48:28.337688Z",
     "start_time": "2025-02-26T18:48:13.380346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e7fc47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16342, 11)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.read_csv('youtube_data.csv')\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12e12cbab0984320",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:48:36.492332Z",
     "start_time": "2025-02-26T18:48:36.477743Z"
    }
   },
   "outputs": [],
   "source": [
    "class YouTubeDataFetcher:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the YouTubeDataFetcher object with the API key and output file.\n",
    "        \"\"\"\n",
    "        self.api_keys = [\n",
    "            os.getenv('YOUTUBE_API_KEY_1'),\n",
    "            os.getenv('YOUTUBE_API_KEY_2'),\n",
    "            os.getenv('YOUTUBE_API_KEY_3'),\n",
    "            os.getenv('YOUTUBE_API_KEY_4'),\n",
    "            os.getenv('YOUTUBE_API_KEY_5'),\n",
    "        ]\n",
    "        self.output_file = 'youtube_data.csv'\n",
    "        self.current_api_key_index = 0  # Keeps track of the current API key index\n",
    "        self.youtube = self.get_youtube_client(self.api_keys[self.current_api_key_index])\n",
    "        self.headers = [\"video_id\", \"title\", \"description\", \"view_count\", \"like_count\",\n",
    "                        \"dislike_count\", \"comment_count\", \"duration\", \"favorite_count\", \"comments\", \"sentiment_score\"]\n",
    "\n",
    "        # Load existing processed video IDs\n",
    "        self.processed_video_ids = self.load_processed_video_ids()\n",
    "\n",
    "        # Initialize CSV file (with headers) if it doesn't exist\n",
    "        self.initialize_csv()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_youtube_client(api_key):\n",
    "        \"\"\"\n",
    "        Initializes and returns the YouTube API client with the given API key.\n",
    "        \"\"\"\n",
    "        return build(\"youtube\", \"v3\", developerKey=api_key)\n",
    "\n",
    "    def load_processed_video_ids(self):\n",
    "        \"\"\"\n",
    "        Loads the list of processed video IDs from the output CSV file.\n",
    "        Returns a set of video IDs that have already been processed.\n",
    "        \"\"\"\n",
    "        processed_ids = set()\n",
    "        try:\n",
    "            df = pd.read_csv(self.output_file)\n",
    "            processed_ids = set(df['video_id'].values)\n",
    "        except FileNotFoundError:\n",
    "            pass  # If the file doesn't exist yet, just return an empty set\n",
    "        return processed_ids\n",
    "\n",
    "    def initialize_csv(self):\n",
    "        \"\"\"\n",
    "        Initializes the CSV file if it doesn't exist, and writes the header.\n",
    "        \"\"\"\n",
    "        if not os.path.exists(self.output_file) or os.stat(self.output_file).st_size == 0:\n",
    "            # Create CSV and write the header if the file doesn't exist or is empty\n",
    "            try:\n",
    "                with open(self.output_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "                    writer = csv.DictWriter(file, fieldnames=self.headers)\n",
    "                    writer.writeheader()\n",
    "            except Exception as e:\n",
    "                print(f\"Error creating file: {str(e)}\")\n",
    "\n",
    "    def fetch_video_data(self, video_id):\n",
    "        \"\"\"\n",
    "        Fetches the video details (views, likes, etc.) and comments for the given video_id.\n",
    "        \"\"\"\n",
    "        attempt = 0\n",
    "        max_attempts = len(self.api_keys) * 2\n",
    "\n",
    "        while attempt < max_attempts:\n",
    "            api_key = self.api_keys[self.current_api_key_index % len(self.api_keys)]  # Rotate API keys continuously\n",
    "            self.youtube = self.get_youtube_client(api_key)  # Re-initialize the client on each key switch\n",
    "            try:\n",
    "                # Get the video details (view count, like count, etc.)\n",
    "                video_response = self.youtube.videos().list(\n",
    "                    part=\"snippet,statistics,contentDetails\",\n",
    "                    id=video_id\n",
    "                ).execute()\n",
    "\n",
    "                if not video_response[\"items\"]:\n",
    "                    raise Exception(f\"No video found for ID: {video_id}\")\n",
    "\n",
    "                video_info = video_response[\"items\"][0]\n",
    "                title = video_info[\"snippet\"][\"title\"]\n",
    "                description = video_info[\"snippet\"].get(\"description\", \"No description\")\n",
    "                view_count = int(video_info[\"statistics\"].get(\"viewCount\", 0))\n",
    "                like_count = int(video_info[\"statistics\"].get(\"likeCount\", 0))\n",
    "                dislike_count = int(video_info[\"statistics\"].get(\"dislikeCount\", 0))\n",
    "                comment_count = int(video_info[\"statistics\"].get(\"commentCount\", 0))\n",
    "                duration = video_info[\"contentDetails\"][\"duration\"]\n",
    "                favorite_count = int(video_info[\"statistics\"].get(\"favoriteCount\", 0))\n",
    "\n",
    "                # Fetching comments (max 100 comments)\n",
    "                comments = []\n",
    "                comment_response = self.youtube.commentThreads().list(\n",
    "                    part=\"snippet\",\n",
    "                    videoId=video_id,\n",
    "                    maxResults=100\n",
    "                ).execute()\n",
    "\n",
    "                for item in comment_response[\"items\"]:\n",
    "                    comment_text = item[\"snippet\"][\"topLevelComment\"][\"snippet\"][\"textDisplay\"]\n",
    "                    comments.append(comment_text)\n",
    "\n",
    "                return {\n",
    "                    \"video_id\": video_id,\n",
    "                    \"title\": title,\n",
    "                    \"description\": description,\n",
    "                    \"view_count\": view_count,\n",
    "                    \"like_count\": like_count,\n",
    "                    \"dislike_count\": dislike_count,\n",
    "                    \"comment_count\": comment_count,\n",
    "                    \"duration\": duration,\n",
    "                    \"favorite_count\": favorite_count,\n",
    "                    \"comments\": comments\n",
    "                }\n",
    "\n",
    "            except Exception as e:\n",
    "                if \"quotaExceeded\" in str(e):\n",
    "                    print(f\"API key {api_key} quota exceeded. Switching to next API key.\")\n",
    "                    attempt += 1\n",
    "                    self.current_api_key_index += 1\n",
    "                    continue  # Try with the next API key\n",
    "                else:\n",
    "                    print(f\"Error fetching data for {video_id}: {str(e)}\")\n",
    "                    return None\n",
    "\n",
    "        print(\"All API keys exhausted, cannot fetch data.\")\n",
    "        return None\n",
    "\n",
    "    @staticmethod\n",
    "    def analyze_sentiment(comments):\n",
    "        \"\"\"\n",
    "        Analyzes the sentiment of the provided comments using VADER Sentiment Analysis.\n",
    "        Returns a sentiment score.\n",
    "        \"\"\"\n",
    "        analyzer = SentimentIntensityAnalyzer()\n",
    "        sentiment_score = 0\n",
    "        num_comments = len(comments)\n",
    "\n",
    "        for comment in comments:\n",
    "            sentiment = analyzer.polarity_scores(comment)\n",
    "            sentiment_score += sentiment['compound']\n",
    "\n",
    "        # Calculate the average sentiment score\n",
    "        if num_comments > 0:\n",
    "            sentiment_score /= num_comments\n",
    "        return sentiment_score\n",
    "\n",
    "    def save_to_csv(self, video_data):\n",
    "        \"\"\"\n",
    "        Saves the fetched video data into the CSV file, appending to it.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(self.output_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "                writer = csv.DictWriter(file, fieldnames=self.headers)\n",
    "                writer.writerow({\n",
    "                    \"video_id\": video_data[\"video_id\"],\n",
    "                    \"title\": video_data[\"title\"],\n",
    "                    \"description\": video_data[\"description\"],\n",
    "                    \"view_count\": video_data[\"view_count\"],\n",
    "                    \"like_count\": video_data[\"like_count\"],\n",
    "                    \"dislike_count\": video_data[\"dislike_count\"],\n",
    "                    \"comment_count\": video_data[\"comment_count\"],\n",
    "                    \"duration\": video_data[\"duration\"],\n",
    "                    \"favorite_count\": video_data[\"favorite_count\"],\n",
    "                    \"comments\": \" | \".join(video_data[\"comments\"]),\n",
    "                    \"sentiment_score\": video_data[\"sentiment_score\"]\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving data for {video_data['video_id']}: {str(e)}\")\n",
    "\n",
    "    def check_if_data_available(self) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the data is available in the output CSV file.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.output_file)\n",
    "            return not df.empty\n",
    "        except FileNotFoundError:\n",
    "            return False\n",
    "\n",
    "    def fetch_and_process_data(self, df):\n",
    "        \"\"\"\n",
    "        Fetches and processes data for each video in the provided DataFrame.\n",
    "        Saves the results into a CSV file.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.check_if_data_available():\n",
    "            print('Data pulled already and available')\n",
    "            return\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            video_id = row['youtubeId']\n",
    "\n",
    "            # Skip the video if it's already processed\n",
    "            if video_id in self.processed_video_ids:\n",
    "                print(f\"{index}. Skipping already processed video: {video_id}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"{index}. Processing video: {video_id}\")\n",
    "\n",
    "            if video_data := self.fetch_video_data(video_id):\n",
    "                # Analyze sentiment of the comments\n",
    "                sentiment_score = self.analyze_sentiment(video_data[\"comments\"])\n",
    "                video_data[\"sentiment_score\"] = sentiment_score\n",
    "\n",
    "                # Save the data to the CSV file\n",
    "                self.save_to_csv(video_data)\n",
    "\n",
    "            # Sleep to avoid rate-limiting\n",
    "            time.sleep(3)\n",
    "\n",
    "        print(f\"Data fetching and saving complete. All data saved in {self.output_file}.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_wordcloud(df):\n",
    "        \"\"\"\n",
    "        Generates a word cloud from the comments of all videos.\n",
    "        \"\"\"\n",
    "        all_comments = \" \".join(df[\"comments\"].dropna())\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_comments)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_graphs(df):\n",
    "        \"\"\"\n",
    "        Generates various plots for video statistics using Plotly Express.\n",
    "        \"\"\"\n",
    "        # Top 10 most viewed videos\n",
    "        top_10 = df.nlargest(10, \"view_count\")\n",
    "        fig = px.bar(top_10, x='title', y='view_count', title='Top 10 Most Viewed Videos')\n",
    "        fig.show()\n",
    "\n",
    "        # Bottom 10 least viewed videos\n",
    "        bottom_10 = df.nsmallest(10, \"view_count\")\n",
    "        fig = px.bar(bottom_10, x='title', y='view_count', title='Bottom 10 Least Viewed Videos')\n",
    "        fig.show()\n",
    "\n",
    "        # Most liked video\n",
    "        most_liked = df.loc[df['like_count'].idxmax()]\n",
    "        fig = px.bar(x=[most_liked['title']], y=[most_liked['like_count']], title='Most Liked Video')\n",
    "        fig.show()\n",
    "\n",
    "        # Least liked video\n",
    "        least_liked = df.loc[df['like_count'].idxmin()]\n",
    "        fig = px.bar(x=[least_liked['title']], y=[least_liked['like_count']], title='Least Liked Video')\n",
    "        fig.show()\n",
    "\n",
    "        # Video with the highest duration\n",
    "        highest_duration = df.loc[df['duration'].idxmax()]\n",
    "        fig = px.bar(x=[highest_duration['title']], y=[highest_duration['duration']],\n",
    "                     title='Video with Highest Duration')\n",
    "        fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e74342f968c1b8b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:48:44.909598Z",
     "start_time": "2025-02-26T18:48:44.893344Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This Clas handles the text in the dataset\n",
    "- it's used to a manipulate textual data\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TextWrangler:\n",
    "    def __init__(self):\n",
    "        self.count = CountVectorizer()\n",
    "        self.tf_idf_ = TfidfVectorizer()\n",
    "        self.spacy_en = spacy.load('en_core_web_sm')\n",
    "        # Download stopwords if not already downloaded\n",
    "        try:\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "        except LookupError:\n",
    "            import nltk\n",
    "            nltk.download('stopwords')\n",
    "            nltk.download('punkt')\n",
    "            self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_digit(text: str) -> str:\n",
    "        \"\"\"\n",
    "        This method removes digit from a text\n",
    "        \"\"\"\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_punctuations(text: str) -> str:\n",
    "        \"\"\"\n",
    "        This method removes punctuation from a text\n",
    "        \"\"\"\n",
    "        return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_text(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean the text and remove punctuations.\n",
    "        \"\"\"\n",
    "        text = re.sub('<.*?>', '', text)\n",
    "        text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "        return text.lower()\n",
    "\n",
    "    @staticmethod\n",
    "    def word_lengthening(text: str) -> str:\n",
    "        \"\"\"\n",
    "        Fix a word by reducing any character repeated three or more times\n",
    "        to just two instances of that character.\n",
    "        \"\"\"\n",
    "        return re.sub(r'(.)\\1{2,}', r'\\1\\1', text.strip())\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenize the given text into words.\n",
    "        \"\"\"\n",
    "        return word_tokenize(text)\n",
    "\n",
    "    def remove_stopwords(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Remove stop words from a given text.\n",
    "        \"\"\"\n",
    "        filtered_text = [word for word in self.tokenize(text) if word.lower() not in self.stop_words]\n",
    "        return ' '.join(filtered_text)\n",
    "\n",
    "    def lemmatizer(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        This method lemmatizes a text by using the spaCy libary\n",
    "        \"\"\"\n",
    "        text = self.spacy_en(text)\n",
    "\n",
    "        # get the lemmatized token from text\n",
    "        lemmatized_tokens = [token.lemma_ for token in text]\n",
    "\n",
    "        # use lemmatized token to form a sentence\n",
    "        return ' '.join(lemmatized_tokens)\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_word_cloud(data, title):\n",
    "        \"\"\"\n",
    "        Generate a graphical representation of the words in the text\n",
    "        \"\"\"\n",
    "        wordcloud = WordCloud(width=900, height=600, max_words=100, background_color='white').generate(data)\n",
    "\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.title(title, fontsize=15)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5b57ee90f467e959",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:49:00.677574Z",
     "start_time": "2025-02-26T18:49:00.628117Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load the CSV containing the video data\n",
    "df = pd.read_csv('vdoLinks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c1af92f63e966101",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:49:01.562460Z",
     "start_time": "2025-02-26T18:49:01.554122Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>youtubeId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K26_sDKnvMU</td>\n",
       "      <td>1</td>\n",
       "      <td>Toy Story (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3LPANjHlPxo</td>\n",
       "      <td>2</td>\n",
       "      <td>Jumanji (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rEnOoWs3FuA</td>\n",
       "      <td>3</td>\n",
       "      <td>Grumpier Old Men (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>j9xml1CxgXI</td>\n",
       "      <td>4</td>\n",
       "      <td>Waiting to Exhale (1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ltwvKLnj1B4</td>\n",
       "      <td>5</td>\n",
       "      <td>Father of the Bride Part II (1995)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     youtubeId  movieId                               title\n",
       "0  K26_sDKnvMU        1                    Toy Story (1995)\n",
       "1  3LPANjHlPxo        2                      Jumanji (1995)\n",
       "2  rEnOoWs3FuA        3             Grumpier Old Men (1995)\n",
       "3  j9xml1CxgXI        4            Waiting to Exhale (1995)\n",
       "4  ltwvKLnj1B4        5  Father of the Bride Part II (1995)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1ab473760ce8db77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:49:04.690174Z",
     "start_time": "2025-02-26T18:49:04.686912Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25623, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e9d0280659a31dbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:49:08.002850Z",
     "start_time": "2025-02-26T18:49:07.973810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25623 entries, 0 to 25622\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   youtubeId  25623 non-null  object\n",
      " 1   movieId    25623 non-null  int64 \n",
      " 2   title      25623 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 600.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3acc2bb45ac5914",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T18:50:57.319697Z",
     "start_time": "2025-02-26T18:50:57.307786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['youtubeId', 'movieId', 'title'], dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d026a6e176d98d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pulled already and available\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of YouTubeDataFetcher and process the data\n",
    "fetcher = YouTubeDataFetcher()\n",
    "fetcher.fetch_and_process_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6811e1637d8a387b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the new CSV containing the downloaded video data\n",
    "df = pd.read_csv('./youtube_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ef9024096a04728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>duration</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>comments</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rEnOoWs3FuA</td>\n",
       "      <td>Grumpier Old Men - Trailer</td>\n",
       "      <td>The more things change, the more they stay the...</td>\n",
       "      <td>231725</td>\n",
       "      <td>220</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>PT1M52S</td>\n",
       "      <td>0</td>\n",
       "      <td>Buena película de comedia romántica | &lt;a href=...</td>\n",
       "      <td>0.192850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2GfZl4kuVNI</td>\n",
       "      <td>Heat Trailer HD (1995)</td>\n",
       "      <td>Director: Michael Mann.\\nCast: Al Pacino, Robe...</td>\n",
       "      <td>1539012</td>\n",
       "      <td>8462</td>\n",
       "      <td>0</td>\n",
       "      <td>681</td>\n",
       "      <td>PT2M28S</td>\n",
       "      <td>0</td>\n",
       "      <td>er muss euch drauf hinweisen ! | Might be one ...</td>\n",
       "      <td>0.336871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id                       title  \\\n",
       "0  rEnOoWs3FuA  Grumpier Old Men - Trailer   \n",
       "1  2GfZl4kuVNI      Heat Trailer HD (1995)   \n",
       "\n",
       "                                         description  view_count  like_count  \\\n",
       "0  The more things change, the more they stay the...      231725         220   \n",
       "1  Director: Michael Mann.\\nCast: Al Pacino, Robe...     1539012        8462   \n",
       "\n",
       "   dislike_count  comment_count duration  favorite_count  \\\n",
       "0              0             13  PT1M52S               0   \n",
       "1              0            681  PT2M28S               0   \n",
       "\n",
       "                                            comments  sentiment_score  \n",
       "0  Buena película de comedia romántica | <a href=...         0.192850  \n",
       "1  er muss euch drauf hinweisen ! | Might be one ...         0.336871  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0e66fc7224ee8d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16342, 11)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04304816",
   "metadata": {},
   "source": [
    "here we can see that the rows are less than 25623 and this is due to the following factors noticed while pulling the data. The factors includes:\n",
    "\n",
    "1.⁠ ⁠not valid id (id does not exist)\n",
    "2.⁠ ⁠⁠comments disabled\n",
    "3.⁠ ⁠⁠the video is not listed again (so the Id might exist but it’s not listed for people to get the data)\n",
    "4.⁠ ⁠⁠the video is made private (this is different from not listed since it’s listed but access is limited to some account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7b60bc729939901f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16342 entries, 0 to 16341\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   video_id         16342 non-null  object \n",
      " 1   title            16342 non-null  object \n",
      " 2   description      15935 non-null  object \n",
      " 3   view_count       16342 non-null  int64  \n",
      " 4   like_count       16342 non-null  int64  \n",
      " 5   dislike_count    16342 non-null  int64  \n",
      " 6   comment_count    16342 non-null  int64  \n",
      " 7   duration         16342 non-null  object \n",
      " 8   favorite_count   16342 non-null  int64  \n",
      " 9   comments         15331 non-null  object \n",
      " 10  sentiment_score  16342 non-null  float64\n",
      "dtypes: float64(1), int64(5), object(5)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "681320f86692c1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'title', 'description', 'view_count', 'like_count',\n",
       "       'dislike_count', 'comment_count', 'duration', 'favorite_count',\n",
       "       'comments', 'sentiment_score'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3de72f1d12e57af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>view_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>dislike_count</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.634200e+04</td>\n",
       "      <td>1.634200e+04</td>\n",
       "      <td>16342.0</td>\n",
       "      <td>16342.000000</td>\n",
       "      <td>16342.0</td>\n",
       "      <td>16342.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.993765e+05</td>\n",
       "      <td>3.204288e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>260.570922</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.195220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.001056e+07</td>\n",
       "      <td>1.096668e+05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2642.896845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.205736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.998800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.410275e+04</td>\n",
       "      <td>5.700000e+01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.369200e+04</td>\n",
       "      <td>1.920000e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.188992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.180360e+05</td>\n",
       "      <td>7.220000e+02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.312450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.485791e+09</td>\n",
       "      <td>1.354636e+07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>264407.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.990800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         view_count    like_count  dislike_count  comment_count  \\\n",
       "count  1.634200e+04  1.634200e+04        16342.0   16342.000000   \n",
       "mean   7.993765e+05  3.204288e+03            0.0     260.570922   \n",
       "std    2.001056e+07  1.096668e+05            0.0    2642.896845   \n",
       "min    0.000000e+00  0.000000e+00            0.0       0.000000   \n",
       "25%    2.410275e+04  5.700000e+01            0.0       6.000000   \n",
       "50%    8.369200e+04  1.920000e+02            0.0      23.000000   \n",
       "75%    3.180360e+05  7.220000e+02            0.0      92.000000   \n",
       "max    2.485791e+09  1.354636e+07            0.0  264407.000000   \n",
       "\n",
       "       favorite_count  sentiment_score  \n",
       "count         16342.0     16342.000000  \n",
       "mean              0.0         0.195220  \n",
       "std               0.0         0.205736  \n",
       "min               0.0        -0.998800  \n",
       "25%               0.0         0.048291  \n",
       "50%               0.0         0.188992  \n",
       "75%               0.0         0.312450  \n",
       "max               0.0         0.990800  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "52c324c1bd220c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Buena película de comedia romántica | <a href=...\n",
       "1    er muss euch drauf hinweisen ! | Might be one ...\n",
       "2    Oxe. No trailer dá logo o spoiler máximo.😂 | B...\n",
       "3    Why does this have the Little Women (1994) sco...\n",
       "4    The last line in the trailer with Dracula was ...\n",
       "Name: comments, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['comments'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57712cf8e9d20c",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "713092c3183b4382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c241017136894c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_wrangler = TextWrangler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0f2d1ac1e646622",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text_wrangler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleanedComments\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m                          \u001b[38;5;241m.\u001b[39mapply(\u001b[43mtext_wrangler\u001b[49m\u001b[38;5;241m.\u001b[39mremove_digit)\n\u001b[1;32m      3\u001b[0m                          \u001b[38;5;241m.\u001b[39mapply(text_wrangler\u001b[38;5;241m.\u001b[39mremove_punctuations)\n\u001b[1;32m      4\u001b[0m                          \u001b[38;5;241m.\u001b[39mapply(text_wrangler\u001b[38;5;241m.\u001b[39mclean_text)\n\u001b[1;32m      5\u001b[0m                          \u001b[38;5;241m.\u001b[39mapply(text_wrangler\u001b[38;5;241m.\u001b[39mword_lengthening)\n\u001b[1;32m      6\u001b[0m                          \u001b[38;5;241m.\u001b[39mapply(text_wrangler\u001b[38;5;241m.\u001b[39mremove_stopwords)\n\u001b[1;32m      7\u001b[0m                          \u001b[38;5;241m.\u001b[39mapply(text_wrangler\u001b[38;5;241m.\u001b[39mlemmatizer))\n\u001b[1;32m      9\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomments\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text_wrangler' is not defined"
     ]
    }
   ],
   "source": [
    "df['cleanedComments'] = (df['comments']\n",
    "                         .apply(text_wrangler.remove_digit)\n",
    "                         .apply(text_wrangler.remove_punctuations)\n",
    "                         .apply(text_wrangler.clean_text)\n",
    "                         .apply(text_wrangler.word_lengthening)\n",
    "                         .apply(text_wrangler.remove_stopwords)\n",
    "                         .apply(text_wrangler.lemmatizer))\n",
    "\n",
    "df['comments'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cd43fa18d1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GenerateGrams:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_ngrams(tokens, n):\n",
    "        ngrams_list = list(ngrams(tokens, n))\n",
    "\n",
    "        return [\" \".join(_gram) for _gram in ngrams_list]\n",
    "\n",
    "    def generate_grams(self, col_df, size):\n",
    "        ngrams_list = []\n",
    "        for _data in col_df:\n",
    "            tokens = _data.split()\n",
    "            if len(tokens) <= size:\n",
    "                continue\n",
    "            ngrams_list.extend(self, self.generate_ngrams(tokens, size))\n",
    "\n",
    "        cnt_ngram = Counter(ngrams_list)\n",
    "        most_common_ngrams = cnt_ngram.most_common(15)\n",
    "\n",
    "        temp_df = pd.DataFrame(most_common_ngrams, columns=['words', 'count'])\n",
    "        temp_df = temp_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "        return temp_df\n",
    "\n",
    "    def plot_n_grams(self, col_df):\n",
    "        unigrams = self.generate_grams(col_df, 1)\n",
    "        bigrams = self.generate_grams(col_df, 2)\n",
    "        trigrams = self.generate_grams(col_df, 3)\n",
    "\n",
    "        # Set plot figure size\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 25))  # 3 rows, 1 column\n",
    "\n",
    "        # Plot Unigrams\n",
    "        ax1.barh(np.arange(len(unigrams['words'])), unigrams['count'], align='center', alpha=.5)\n",
    "        ax1.set_title('Unigrams')\n",
    "        ax1.set_yticks(np.arange(len(unigrams['words'])))\n",
    "        ax1.set_yticklabels(unigrams['words'])\n",
    "        ax1.set_xlabel('Count')\n",
    "\n",
    "        # Plot Bigrams\n",
    "        ax2.barh(np.arange(len(bigrams['words'])), bigrams['count'], align='center', alpha=.5)\n",
    "        ax2.set_title('Bigrams')\n",
    "        ax2.set_yticks(np.arange(len(bigrams['words'])))\n",
    "        ax2.set_yticklabels(bigrams['words'])\n",
    "        ax2.set_xlabel('Count')\n",
    "\n",
    "        # Plot Trigrams\n",
    "        ax3.barh(np.arange(len(trigrams['words'])), trigrams['count'], align='center', alpha=.5)\n",
    "        ax3.set_title('Trigrams')\n",
    "        ax3.set_yticks(np.arange(len(trigrams['words'])))\n",
    "        ax3.set_yticklabels(trigrams['words'])\n",
    "        ax3.set_xlabel('Count')\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout for better spacing\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f6335a215fd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the class\n",
    "gram_generator = GenerateGrams()\n",
    "\n",
    "# Generate n-grams and plot them\n",
    "gram_generator.plot_n_grams(df['cleanedComments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973857230a4d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TextSentimentAnalysis Class provides the sentiment for the input text in its raw form\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class TextSentimentAnalysis:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initialize vader sentiment analyzer\n",
    "        \"\"\"\n",
    "        self.sid = SentimentIntensityAnalyzer()\n",
    "\n",
    "    def get_sentiment_via_vader(self, text: str) -> float:\n",
    "        \"\"\"\n",
    "        return the compound score\n",
    "        \"\"\"\n",
    "        # Using compound score as a single sentiment score\n",
    "        return self.sid.polarity_scores(text)['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9251b515f6132240",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentiment_analysis = TextSentimentAnalysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b389566ba2b11b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['commentsSentiment'] = df['comments'].apply(text_sentiment_analysis.get_sentiment_via_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66a12e887343511",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleanedCommentsSentiment'] = df['cleanedComments'].apply(text_sentiment_analysis.get_sentiment_via_vader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b867df57e4345051",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f4be01d706f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the word cloud and plots\n",
    "fetcher.generate_wordcloud(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0f392c7d846432",
   "metadata": {},
   "outputs": [],
   "source": [
    "fetcher.generate_graphs(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
